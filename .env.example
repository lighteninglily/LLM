# =============================================================================
# Local AI Server - Environment Configuration
# =============================================================================
# Copy this file to .env and customize as needed
# 
# Usage:
#   cp .env.example .env
#   nano .env
# =============================================================================

# -----------------------------------------------------------------------------
# HuggingFace Configuration (REQUIRED for gated models like Llama-2)
# -----------------------------------------------------------------------------
# Get your token from: https://huggingface.co/settings/tokens
# For gated models, also accept the license at the model page
HF_TOKEN=

# -----------------------------------------------------------------------------
# LLM Model Configuration
# -----------------------------------------------------------------------------
# Model to use (HuggingFace model ID)
# For 1x RTX 5090 (32GB VRAM), recommended:
#   - Qwen/Qwen3-32B-Instruct (best quality, SOTA 2026)
#   - google/gemma-3-27b (efficient, fast)
#   - casperhansen/llama-3.3-70b-instruct-awq (high quality, tight fit)
#
# For coding tasks:
#   - Qwen/Qwen3-Coder-32B
#   - deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
LLM_MODEL=Qwen/Qwen3-32B-Instruct

# Model name for AnythingLLM (usually last part of model ID)
LLM_MODEL_NAME=Qwen3-32B-Instruct

# Tensor parallelism - number of GPUs to split model across
# Set to 1 for single GPU (RTX 5090)
TENSOR_PARALLEL_SIZE=1

# Maximum context length (tokens)
# Higher = more context but more VRAM
# 4096 is safe for 48GB, can try 8192 for smaller models
MAX_MODEL_LEN=4096

# GPU memory utilization (0.0 - 1.0)
# 0.85 leaves headroom for KV cache, lower if OOM errors
GPU_MEMORY_UTIL=0.85

# -----------------------------------------------------------------------------
# Server Ports
# -----------------------------------------------------------------------------
# vLLM API port (OpenAI-compatible)
VLLM_PORT=8000

# AnythingLLM web interface port
ANYTHINGLLM_PORT=3001

# -----------------------------------------------------------------------------
# Storage Paths
# -----------------------------------------------------------------------------
# Where to store downloaded models
MODELS_DIR=~/.local-ai-server/models

# Where to store application data (documents, chats, etc.)
DATA_DIR=~/.local-ai-server/data

# -----------------------------------------------------------------------------
# Authentication (optional)
# -----------------------------------------------------------------------------
# Authentication (SECURITY: false = requires login)
# Set to false to require login for AnythingLLM (RECOMMENDED)
PASSWORDLESS_AUTH=false

# If PASSWORDLESS_AUTH=false, set this token
AUTH_TOKEN=

# -----------------------------------------------------------------------------
# Optional: Fallback API Keys (for hybrid local/cloud setups)
# -----------------------------------------------------------------------------
# Not used by default, but AnythingLLM can use these as fallbacks
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# -----------------------------------------------------------------------------
# Advanced: vLLM Options
# -----------------------------------------------------------------------------
# Uncomment and modify in docker-compose.yml command section if needed
#
# --quantization awq          # or gptq, squeezellm, etc.
# --dtype float16             # or bfloat16, auto
# --enforce-eager             # disable CUDA graphs (more stable)
# --disable-log-requests      # quieter logs
# --max-num-batched-tokens    # for high concurrency
# --max-num-seqs              # max concurrent sequences
