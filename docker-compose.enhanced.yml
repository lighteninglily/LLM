# docker-compose.enhanced.yml
# ENHANCED configuration with multiple interfaces and code execution
#
# Includes:
# - vLLM (LLM inference)
# - AnythingLLM (RAG + document chat)
# - Open WebUI (polished ChatGPT-like interface)
# - JupyterLab (code execution + data science)
#
# Usage:
#   docker compose -f docker-compose.enhanced.yml up -d
#
# Access points:
#   - Open WebUI: http://localhost:3000 (best general chat)
#   - AnythingLLM: http://localhost:3001 (best for RAG/documents)
#   - JupyterLab: http://localhost:8888 (code execution)
#   - vLLM API: http://localhost:8000 (direct API)

version: '3.8'

services:
  # ==========================================================================
  # vLLM - High-performance LLM Inference
  # ==========================================================================
  vllm:
    image: vllm/vllm-openai:latest
    container_name: local-ai-vllm
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN:-}
      - VLLM_LOGGING_LEVEL=INFO
    volumes:
      - ${MODELS_DIR:-~/.local-ai-server/models}:/root/.cache/huggingface
      - /dev/shm:/dev/shm
    ports:
      - "${VLLM_PORT:-8000}:8000"
    command: >
      --model ${LLM_MODEL:-Qwen/Qwen3-32B-Instruct}
      --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1}
      --max-model-len ${MAX_MODEL_LEN:-8192}
      --gpu-memory-utilization ${GPU_MEMORY_UTIL:-0.85}
      --dtype auto
      --trust-remote-code
      --enable-chunked-prefill
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 600s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # ==========================================================================
  # Open WebUI - Best ChatGPT-like Interface
  # ==========================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: local-ai-openwebui
    volumes:
      - ${DATA_DIR:-~/.local-ai-server/data}/open-webui:/app/backend/data
    ports:
      - "${OPENWEBUI_PORT:-3000}:8080"
    environment:
      - OPENAI_API_BASE_URL=http://vllm:8000/v1
      - OPENAI_API_KEY=not-needed
      - WEBUI_AUTH=${WEBUI_AUTH:-true}
      - WEBUI_NAME=Lightning Lily AI
      - ENABLE_RAG_WEB_SEARCH=true
      - RAG_EMBEDDING_ENGINE=openai
      - RAG_OPENAI_API_BASE_URL=http://vllm:8000/v1
      - PDF_EXTRACT_IMAGES=true
      - CHUNK_SIZE=1000
      - CHUNK_OVERLAP=100
    depends_on:
      vllm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ==========================================================================
  # AnythingLLM - Best RAG + Document Management
  # ==========================================================================
  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: local-ai-anythingllm
    volumes:
      - ${DATA_DIR:-~/.local-ai-server/data}/anythingllm:/app/server/storage
      - ${DATA_DIR:-~/.local-ai-server/data}/documents:/app/server/storage/documents
    ports:
      - "${ANYTHINGLLM_PORT:-3001}:3001"
    environment:
      - STORAGE_DIR=/app/server/storage
      - LLM_PROVIDER=generic-openai
      - GENERIC_OPEN_AI_BASE_PATH=http://vllm:8000/v1
      - GENERIC_OPEN_AI_MODEL_PREF=${LLM_MODEL_NAME:-Qwen3-32B-Instruct}
      - GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT=${MAX_MODEL_LEN:-8192}
      - GENERIC_OPEN_AI_API_KEY=not-needed
      - EMBEDDING_ENGINE=native
      - EMBEDDING_MODEL_PREF=nomic-embed-text
      - VECTOR_DB=lancedb
      - PASSWORDLESS_AUTH=${PASSWORDLESS_AUTH:-false}
    depends_on:
      vllm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # ==========================================================================
  # JupyterLab - Code Execution + Data Science
  # ==========================================================================
  jupyter:
    image: jupyter/datascience-notebook:latest
    container_name: local-ai-jupyter
    user: root
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=${JUPYTER_TOKEN:-lightninglily}
      - GRANT_SUDO=yes
      - NB_UID=1000
      - NB_GID=1000
    volumes:
      - ${DATA_DIR:-~/.local-ai-server/data}/notebooks:/home/jovyan/work
      - ${DATA_DIR:-~/.local-ai-server/data}/documents:/home/jovyan/data:ro
      - ${DATA_DIR:-~/.local-ai-server/data}/analysis:/home/jovyan/analysis
    ports:
      - "${JUPYTER_PORT:-8888}:8888"
    command: >
      bash -c "
        pip install openai httpx pandas matplotlib seaborn plotly openpyxl xlrd &&
        start-notebook.sh --NotebookApp.token='${JUPYTER_TOKEN:-lightninglily}'
      "
    restart: unless-stopped

  # ==========================================================================
  # Ollama - Alternative lighter-weight inference (optional)
  # ==========================================================================
  # Uncomment if you want Ollama as backup for smaller models
  #
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: local-ai-ollama
  #   volumes:
  #     - ${DATA_DIR:-~/.local-ai-server/data}/ollama:/root/.ollama
  #   ports:
  #     - "11434:11434"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   restart: unless-stopped

networks:
  default:
    name: local-ai-network
    driver: bridge
